#How to avoid Over fitting of ML model 

The main reason for model under or over fitting is the bias and variance. Bias in a model means that the model is missing important features of the datasets. 
This means that the data is too basic. Bias is measured by the difference between the expected predictions of the model and the true values we are trying to predict. 
If the difference is narrow, then the model has low bias. If the difference is wide, then the model has a high bias. When a model has a high bias, it is underfitted. 
Underfitted means that the model is not capturing enough difference in the features of the data, and therefore, the model performs poorly on the training data. 

To help overcome bias and variance errors, you can use the following:
[1] Cross Validation: Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. 
Cross-validation should be used to detect overfitting. 

[2] Increase data

[3] Do Regularization of weights

[4] Use very simple model architectures. ***If the model is underfitting, the model might be too simple***.  

[5] Use dimensionality reduction techniques on input data, before training model.

[6] ***Early Stopping***: End training early so that the model does not memorize the data.
